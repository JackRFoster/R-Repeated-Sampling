---
title: "Lab 10"
author: 'Jack Foster'
date: "11/10ÃŸ/2022"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In parctice, we will use qf() and qt() to compute the quantiles of F and t distributions, with different parameters: probability and degree of freedoms. The following two questions aim to help you understand the nature of the two functions.

# Quantiles from the F distribution

0.  Set the random seed to 1.

```{r}
set.seed(1)
```

1.  Generate 1000 random samples from the F distribution with degree of freedom (5, 30).

```{r}
sampf <-rf(1000, 5, 30)
```

2.  Plot the density of the 1000 random samples using the function plot(density()).

```{r}
plot(density(sampf))
```

3.  Get the 0.95 quantile of the 1000 samples.

```{r}
quantile(sampf, .95)
```

4.  Get the 0.95 quantile of the F distribution with degree of freedom (5, 30).

```{r}
qf(.95, 5, 30)
```

5.  Are the two numbers in step 3 and 4 close to each other? What if you get the 0.95 quantile of 10000 random samples from the sample F distribution? Compare the results.

Yes, as the number of samples taken from the distribution increases the sample distribution more closely approximates the true F distribution.

```{r}
sampf2 <-rf(10000, 5, 30)
# plot(dist(sampf2)) Computer Can't run
quantile(sampf2, .95)
```

# Quantiles from the t distribution

0.  Set the random seed to 1.
```{r}
set.seed(1)
```

1.  Generate 1000 random samples from the t distribution with degree of freedom 20.
```{r}
sampt <-rt(1000, 5, 30)
```
2.  Plot the density of the 1000 random samples using the function plot(density()).
```{r}
plot(density(sampt))
```

3.  Get the 0.95 quantile of the 1000 samples.
```{r}
quantile(sampt, .95)
```
4.  Get the 0.95 quantile of the the t distribution with degree of freedom 20.
```{r}
qt(.95, 20)
```

5.  Are the two numbers in step 3 and 4 close to each other? What if you get the 0.95 quantile of 10000 random samples from the sample t distribution? Compare the results.
```{r}
sampt2 <-rt(10000, 5, 30)
# plot(dist(sampf2)) Computer Can't run
quantile(sampt2, .95)
```

Yes, they are close.  As the number of samples taken from the distribution increases the sample distribution more closely approximates the true T distribution.

# NFL Data

In this question, you will answer the question using the NFL home advantage data. Denote the population mean of home advantage points from year 2015 to year 2019 as $\mu = (\mu_1,\mu_2,\mu_3,\mu_4,\mu_5)$.

```{r}
NFL <- read.csv(file.choose(), header=T)
```

1.  What is the sample estimate of $(\mu_1,\mu_2,\dots, \mu_5)$? Use the function colMeans().

```{r}
colMeansNFL <- colMeans(NFL)
```

2.  Report the covariance matrix of the points of the home advantage from year 2015 to 2019.

```{r}
cov.NFL <-cov(NFL)
```

3.  Test the null hypothesis that $(\mu_1,\mu_2,\mu_3,\mu_4,\mu_5)$ are equal to $(1,3,1,1,0)$ Apply the one-sample Hotelling T-squared test. What is the value of the test statistic? What is the p-value? If type-I error is 0.05, what is your conclusion?

```{r}

```

4.  With 95% confidence, is $(1,2,2,3,0)$ a plausible value for $(\mu_1,\mu_2,\mu_3,\mu_4,\mu_5)$? Use the formula in class to decide whether this vector is inside the 95% confidence region.

```{r}
n <- nrow(NFL)
p <- ncol(NFL)
mu <- c(1,2,2,3,0)

Left <- n*t(colMeansNFL-mu)%*%solve(cov.NFL)%*%(colMeansNFL-mu)
#3.4479
#Critical Value
CV <- (n-1)*(p/(p-1))*qf(0.95, p , n-p)
#Fail to reject the null
```

5.  Report the $T^2$ simultaneous 95% confidence intervals for the mean points of the home advantage from year 2015 to 2019. You will report five confidence intervals.
```{r}
for (i in 1:5) {
  upper_bound <- mean(NFL[,i]) + 
    sqrt((p*(n-1)/(n-p))*qf(.95,p, n-p))*sqrt(cov.NFL[i,i]/n)
  lower_bound <- mean(NFL[,i]) - 
    sqrt((p*(n-1)/(n-p))*qf(.95,p, n-p))*sqrt(cov.NFL[i,i]/n)
  print(rbind(lower_bound, upper_bound))
}
  
```

6.  Report the Bonferroni simultaneous 95% confidence intervals for the mean points of the home advantage from year 2015 to 2019. You will also report five confidence intervals.

```{r}
for (i in 1:5) {
  upper_bound2 <- mean(NFL[,i]) +
    qt((1-.05)/(2*p), n-1)*sqrt(cov.NFL[i,i]/n)
  lower_bound2 <- mean(NFL[,i]) -
    qt((1-.05)/(2*p), n-1)*sqrt(cov.NFL[i,i]/n)
  print(rbind(lower_bound2, upper_bound2))
}
```

7.  Next, we will construct confidence intervals for only one linear combination of $\mu$. Report both the $T^2$ 95% confidence intervals and the Bonferroni confidence intervals for $\mu_2 - \mu_5$. Based on the confidence intervals, what is your interpretation for the home advantage in year 2016 and 2019?
```{r}
## T2
upper_T2 <- mean(NFL[,2])-mean(NFL[,5]) +
  sqrt((p*(n-1)/(n-p))*qf(.95,  p, n-p))*sqrt(cov.NFL[2,2] - 2*cov.NFL[2,5]+cov.NFL[5,5])

lower_T2 <- mean(NFL[,2])-mean(NFL[,5]) -
  sqrt((p*(n-1)/(n-p))*qf(.95,  p, n-p))*sqrt(cov.NFL[2,2] - 2*cov.NFL[2,5]+cov.NFL[5,5])

print(rbind(lower_T2, upper_T2))


##Bonferroni
upper_bonf <- mean(NFL[,2])-mean(NFL[,5]) +
  qt(0.975, n-1)*sqrt((cov.NFL[2,2] - 2*cov.NFL[2,5]+cov.NFL[5,5])/n)

lower_bonf <- mean(NFL[,2])-mean(NFL[,5]) -
  qt(0.975, n-1)*sqrt((cov.NFL[2,2] - 2*cov.NFL[2,5]+cov.NFL[5,5])/n)

```

8.  Report both the $T^2$ 95% confidence intervals and the Bonferroni confidence intervals for $\mu_3 - \mu_4$. Based on the confidence intervals, what is your interpretation for the home advantage in year 2017 and 2018

```{r}
## T2
upper <- mean(NFL[,3])-mean(NFL[,4]) +
  sqrt((p*(n-1)/(n-p))*qf(.95,  p, n-p))*sqrt(cov.NFL[3,3] - 2*cov.NFL[3,4]+cov.NFL[4,4])

lower <- mean(NFL[,3])-mean(NFL[,4]) -
  sqrt((p*(n-1)/(n-p))*qf(.95,  p, n-p))*sqrt(cov.NFL[3,3] - 2*cov.NFL[3,4]+cov.NFL[4,4])

print(rbind(lower, upper))

```

9.  Next, we will construct simultaneous confidence intervals for multiple linear combinations of $\mu$. Report the simultaneous confidence intervals for $\mu_1-\mu_5$, $\mu_2-\mu_5$, $\mu_3-\mu_5$, $\mu_4-\mu_5$ using both the $T^2$ and Bonferroni method.
```{r}
for (i in 1:4) {
  
  upper <- mean(NFL[,i])-mean(NFL[,5]) +
  sqrt((p*(n-1)/(n-p))*qf(.95,  p, n-p))*sqrt(cov.NFL[i,i] - 2*cov.NFL[i,5]+cov.NFL[5,5])

lower <- mean(NFL[,i])-mean(NFL[,5]) -
  sqrt((p*(n-1)/(n-p))*qf(.95,  p, n-p))*sqrt(cov.NFL[i,i] - 2*cov.NFL[i,5]+cov.NFL[5,5])

print(rbind(lower, upper))
  
}

```

Refer to page 28

10. Lastly, we test the null hypothesis of $H_0: C\mu=0$, where $C$ is a matrix given below.

```{r}
C = matrix(c(1,0,0,0,0,1,0,0, 0,0,1,0,0,0,0,1), 4, 4, byrow = T)
C
```

11. Calculate the $T^2$ test statistic using the formula in the slides. What is the value of the test statistic?
```{r}
#n*t(C%*%colMeansNFL)%*%solve(C%*%cov.NFL%*%t(C)%*%(C%*%colMeansNFL))

(n-1)*(p-1)/(n-p+1)*qf(0.95, p-1, n-p+1)
```
12. If $\alpha = 0.05$, should you reject the null hypothesis? State the reasons.

Given the p value is not significant we fail to reject the null hypothesis at the .05 level.  I.E. the first statistic is smaller than the f statistic

